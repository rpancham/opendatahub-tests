from typing import cast, Any, Generator
import copy

import pytest
from syrupy.extensions.json import JSONSnapshotExtension
from pytest_testconfig import config as py_config

from kubernetes.dynamic import DynamicClient
from ocp_resources.namespace import Namespace
from ocp_resources.serving_runtime import ServingRuntime
from ocp_resources.inference_service import InferenceService
from ocp_resources.pod import Pod
from ocp_resources.secret import Secret
from ocp_resources.template import Template
from ocp_resources.service_account import ServiceAccount

from tests.model_serving.model_runtime.triton.constant import (
    PREDICT_RESOURCES,
    RUNTIME_MAP,
    TEMPLATE_MAP,
    TEMPLATE_FILE_PATH,
)
from tests.model_serving.model_runtime.triton.basic_model_deployment.utils import kserve_s3_endpoint_secret

from utilities.constants import (
    KServeDeploymentType,
    Labels,
    RuntimeTemplates,
    Protocols,
)
from utilities.inference_utils import create_isvc
from utilities.infra import get_pods_by_isvc_label
from utilities.serving_runtime import ServingRuntimeFromTemplate

from simple_logger.logger import get_logger


LOGGER = get_logger(name=__name__)


@pytest.fixture(scope="session")
def root_dir(pytestconfig: pytest.Config) -> Any:
    """
    Provides the root directory path of the pytest project for the entire test session.

    Args:
        pytestconfig (pytest.Config): The pytest configuration object.

    Returns:
        Any: The root path of the pytest project.
    """
    return pytestconfig.rootpath


@pytest.fixture(scope="class")
def triton_grpc_serving_runtime_template(admin_client: DynamicClient) -> Generator[Template, None, None]:
    """
    Provides a gRPC serving runtime Template for Triton within the test class scope.

    Args:
        admin_client (DynamicClient): Kubernetes dynamic client.

    Yields:
        Template: The loaded gRPC serving runtime Template.
    """
    grpc_template_yaml = TEMPLATE_FILE_PATH.get(Protocols.GRPC)
    with Template(
        client=admin_client,
        yaml_file=grpc_template_yaml,
        namespace=py_config["applications_namespace"],
    ) as tp:
        yield tp


@pytest.fixture(scope="class")
def triton_rest_serving_runtime_template(admin_client: DynamicClient) -> Generator[Template, None, None]:
    """
    Provides a REST serving runtime Template for Triton within the test class scope.

    Args:
        admin_client (DynamicClient): Kubernetes dynamic client.

    Yields:
        Template: The loaded REST serving runtime Template.
    """
    rest_template_yaml = TEMPLATE_FILE_PATH.get(Protocols.REST)
    with Template(
        client=admin_client,
        yaml_file=rest_template_yaml,
        namespace=py_config["applications_namespace"],
    ) as tp:
        yield tp


@pytest.fixture(scope="class")
def protocol(request: pytest.FixtureRequest) -> str:
    """
    Provides the protocol type parameter for the test class.

    Args:
        request (pytest.FixtureRequest): The pytest fixture request object.

    Returns:
        str: The protocol type specified in the test parameter.
    """
    return request.param["protocol_type"]


@pytest.fixture(scope="class")
def triton_serving_runtime(
    request: pytest.FixtureRequest,
    admin_client: DynamicClient,
    model_namespace: Namespace,
    triton_runtime_image: str,
    protocol: str,
) -> Generator[ServingRuntime, None, None]:
    """
    Provides a ServingRuntime resource for Triton with the specified protocol and deployment type.

    Args:
        request (pytest.FixtureRequest): Pytest fixture request containing parameters.
        admin_client (DynamicClient): Kubernetes dynamic client.
        model_namespace (Namespace): Kubernetes namespace for model deployment.
        triton_runtime_image (str): The container image for the Triton runtime.
        protocol (str): The protocol to use (e.g., REST or GRPC).

    Yields:
        ServingRuntime: An instance of the Triton ServingRuntime configured as per parameters.
    """
    template_name = TEMPLATE_MAP.get(protocol, RuntimeTemplates.TRITON_REST)
    with ServingRuntimeFromTemplate(
        client=admin_client,
        name=RUNTIME_MAP.get(protocol, "triton-runtime"),
        namespace=model_namespace.name,
        template_name=template_name,
        deployment_type=request.param["deployment_type"],
        runtime_image=triton_runtime_image,
    ) as model_runtime:
        yield model_runtime


@pytest.fixture(scope="class")
def triton_inference_service(
    request: pytest.FixtureRequest,
    admin_client: DynamicClient,
    model_namespace: Namespace,
    triton_serving_runtime: ServingRuntime,
    s3_models_storage_uri: str,
    triton_model_service_account: ServiceAccount,
) -> Generator[InferenceService, Any, Any]:
    """
    Creates and yields a configured InferenceService instance for Triton testing.

    Args:
        request (pytest.FixtureRequest): Pytest fixture request containing test parameters.
        admin_client (DynamicClient): Kubernetes dynamic client.
        model_namespace (Namespace): Kubernetes namespace for model deployment.
        triton_serving_runtime (ServingRuntime): The Triton ServingRuntime instance.
        s3_models_storage_uri (str): URI for the S3 storage location of models.
        triton_model_service_account (ServiceAccount): Service account for the model.

    Yields:
        InferenceService: A configured InferenceService resource.
    """
    params = request.param
    service_config = {
        "client": admin_client,
        "name": params.get("name"),
        "namespace": model_namespace.name,
        "runtime": triton_serving_runtime.name,
        "storage_uri": s3_models_storage_uri,
        "model_format": triton_serving_runtime.instance.spec.supportedModelFormats[0].name,
        "model_service_account": triton_model_service_account.name,
        "deployment_mode": params.get("deployment_type", KServeDeploymentType.RAW_DEPLOYMENT),
        "external_route": params.get("enable_external_route", False),
    }

    gpu_count = params.get("gpu_count", 0)
    timeout = params.get("timeout")
    # timeout = 10*60

    min_replicas = params.get("min-replicas")

    resources = copy.deepcopy(cast(dict[str, dict[str, str]], PREDICT_RESOURCES["resources"]))
    if gpu_count > 0:
        identifier = Labels.Nvidia.NVIDIA_COM_GPU
        resources["requests"][identifier] = gpu_count
        resources["limits"][identifier] = gpu_count
        service_config["volumes"] = PREDICT_RESOURCES["volumes"]
        service_config["volumes_mounts"] = PREDICT_RESOURCES["volume_mounts"]
    service_config["resources"] = resources

    if timeout:
        service_config["timeout"] = timeout

    if min_replicas:
        service_config["min_replicas"] = min_replicas

    with create_isvc(**service_config) as isvc:
        yield isvc


@pytest.fixture(scope="class")
def triton_model_service_account(admin_client: DynamicClient, kserve_s3_secret: Secret) -> ServiceAccount:
    """
    Creates and yields a ServiceAccount linked to the provided S3 secret for Triton models.

    Args:
        admin_client (DynamicClient): Kubernetes dynamic client.
        kserve_s3_secret (Secret): The Kubernetes secret containing S3 credentials.

    Yields:
        ServiceAccount: A ServiceAccount configured with access to the S3 secret.
    """
    with ServiceAccount(
        client=admin_client,
        namespace=kserve_s3_secret.namespace,
        name="triton-models-bucket-sa",
        secrets=[{"name": kserve_s3_secret.name}],
    ) as sa:
        yield sa


@pytest.fixture(scope="class")
def kserve_s3_secret(
    admin_client: DynamicClient,
    model_namespace: Namespace,
    aws_access_key_id: str,
    aws_secret_access_key: str,
    models_s3_bucket_region: str,
    models_s3_bucket_endpoint: str,
) -> Secret:
    """
    Creates and yields a Kubernetes Secret configured for S3 access in KServe.

    Args:
        admin_client (DynamicClient): Kubernetes dynamic client.
        model_namespace (Namespace): Namespace where the secret will be created.
        aws_access_key_id (str): AWS access key ID.
        aws_secret_access_key (str): AWS secret access key.
        models_s3_bucket_region (str): AWS S3 bucket region.
        models_s3_bucket_endpoint (str): AWS S3 bucket endpoint URL.

    Yields:
        Secret: A Kubernetes Secret configured with the provided AWS credentials and S3 endpoint.
    """
    with kserve_s3_endpoint_secret(
        admin_client=admin_client,
        name="triton-models-bucket-secret",
        namespace=model_namespace.name,
        aws_access_key=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        aws_s3_region=models_s3_bucket_region,
        aws_s3_endpoint=models_s3_bucket_endpoint,
    ) as secret:
        yield secret


@pytest.fixture
def triton_response_snapshot(snapshot: Any) -> Any:
    """
    Provides a snapshot fixture configured to use JSONSnapshotExtension for Triton responses.

    Args:
        snapshot (Any): The base snapshot fixture.

    Returns:
        Any: Snapshot fixture extended with JSONSnapshotExtension.
    """
    return snapshot.use_extension(extension_class=JSONSnapshotExtension)


@pytest.fixture
def triton_pod_resource(
    admin_client: DynamicClient,
    triton_inference_service: InferenceService,
) -> Pod:
    """
    Retrieves the first Kubernetes Pod associated with the given Triton InferenceService.

    Args:
        admin_client (DynamicClient): Kubernetes dynamic client.
        triton_inference_service (InferenceService): The Triton InferenceService resource.

    Returns:
        Pod: The first Pod found for the InferenceService.

    Raises:
        RuntimeError: If no pods are found for the specified InferenceService.
    """
    pods = get_pods_by_isvc_label(client=admin_client, isvc=triton_inference_service)
    if not pods:
        raise RuntimeError(f"No pods found for InferenceService {triton_inference_service.name}")
    return pods[0]